Perfect üëç
I‚Äôll now **REWRITE UNIT 4 answers with DEEP THEORY**, exactly how **SPPU evaluators expect for full 9/9**, with:

* Clear **definitions**
* **Detailed working**
* **Algorithms / formulas**
* **Advantages + limitations**
* **Examples**
* **Proper structure**

Take this as **FINAL WRITE-UP** material.

---

# ‚úÖ **UNIT 4 ‚Äì Classification & Retrieval Models (DETAILED THEORY)**

---

## üîπ **Q3 (a) ‚Äì Explain Categorization and Filtering with suitable examples. [9 Marks]**

---

## **1Ô∏è‚É£ Categorization**

### **Definition (1 Mark)**

**Categorization** is the process of automatically assigning documents to **predefined classes or categories** based on their content.

---

### **Need for Categorization (1 Mark)**

* Organizes large document collections
* Enables efficient search and browsing
* Supports personalized services

---

### **Working of Categorization (3 Marks)**

1. **Document Representation**

   * Convert document into feature vector
   * Common features: TF-IDF, keywords, n-grams

2. **Training Phase**

   * Use labeled documents
   * Apply machine learning classifiers

3. **Classification Phase**

   * New document classified into category with highest score/probability

---

### **Types of Categorization (1 Mark)**

* **Single-label**: One category per document
* **Multi-label**: Multiple categories per document

---

### **Examples of Categorization (2 Marks)**

* **Email classification**: Spam / Non-spam
* **News classification**: Sports, Politics, Technology

---

## **2Ô∏è‚É£ Filtering**

### **Definition (1 Mark)**

**Filtering** is the process of selecting documents **relevant to a user‚Äôs interests** and removing irrelevant ones.

---

### **Working of Filtering (2 Marks)**

* Uses **user profile**
* Compares incoming documents with profile
* Delivers only relevant documents

---

### **Types of Filtering (1 Mark)**

* Content-based filtering
* Rule-based filtering

---

### **Examples of Filtering (2 Marks)**

* Email filters blocking promotional mails
* News apps showing preferred topics

---

### **Conclusion (¬Ω Mark)**

Categorization structures information, while filtering personalizes information delivery.

---

---

## üîπ **Q3 (b) ‚Äì Explain Information-Theoretic Model in detail. [8/9 Marks]**

---

### **Definition (1 Mark)**

The **Information-Theoretic Model** applies concepts of **information theory** to information retrieval, where relevance is treated as **reduction in uncertainty**.

---

### **Basic Concepts (2 Marks)**

* **Entropy**: Measure of uncertainty
  [
  H(X) = -\sum p(x)\log p(x)
  ]

* **Information Gain**: Reduction in entropy

---

### **Working Principle (3 Marks)**

* Query and document represented as probability distributions
* Relevance measured using **divergence**
* Common metric:

  * **Kullback‚ÄìLeibler Divergence (KL-Divergence)**

[
D_{KL}(P||Q) = \sum P(x)\log \frac{P(x)}{Q(x)}
]

Lower divergence ‚áí Higher relevance

---

### **Advantages (2 Marks)**

* Strong mathematical foundation
* Models uncertainty explicitly
* Effective for probabilistic retrieval

---

### **Limitations (1 Mark)**

* Probability estimation is complex
* Computationally expensive

---

### **Conclusion (¬Ω Mark)**

The Information-Theoretic Model ranks documents by minimizing uncertainty between query and document distributions.

---

---

## üîπ **Q4 (a) ‚Äì Explain Probabilistic Classifiers & Generalized Linear Models. [9 Marks]**

---

## **1Ô∏è‚É£ Probabilistic Classifiers**

### **Definition (1 Mark)**

Probabilistic classifiers assign a class to a document based on **posterior probability**.

---

### **Bayes Theorem (2 Marks)**

[
P(C|D) = \frac{P(D|C)P(C)}{P(D)}
]

Where:

* C = class
* D = document

---

### **Working (2 Marks)**

1. Compute prior probability P(C)
2. Compute likelihood P(D|C)
3. Compute posterior probability
4. Select class with maximum probability

---

### **Example (1 Mark)**

* **Naive Bayes Classifier**

  * Assumes conditional independence of features

---

### **Advantages (1 Mark)**

* Simple and fast
* Works well with high-dimensional data

---

## **2Ô∏è‚É£ Generalized Linear Models (GLM)**

### **Definition (1 Mark)**

GLMs generalize linear regression to support **non-normal output variables**.

---

### **Components of GLM (2 Marks)**

1. **Random Component** ‚Äì probability distribution
2. **Systematic Component** ‚Äì linear predictor
3. **Link Function** ‚Äì connects mean to predictor

---

### **Example (1 Mark)**

* **Logistic Regression** for binary classification

---

### **Conclusion (¬Ω Mark)**

Probabilistic classifiers and GLMs provide robust classification mechanisms for IR systems.

---

---

## üîπ **Q4 (b) ‚Äì Describe Language Models and Smoothing. [8/9 Marks]**

---

## **1Ô∏è‚É£ Language Models**

### **Definition (1 Mark)**

A **Language Model** assigns a probability to a sequence of words.

---

### **Language Model in IR (2 Marks)**

* Documents ranked by:
  [
  P(Q|D)
  ]

Higher probability ‚áí More relevant document

---

### **Types of Language Models (2 Marks)**

* Unigram Model
* Bigram Model
* Trigram Model

---

### **Problem: Zero Probability (1 Mark)**

Unseen words in document lead to zero probability.

---

## **2Ô∏è‚É£ Smoothing Techniques**

### **Definition (1 Mark)**

Smoothing adjusts probabilities to avoid zero values.

---

### **Types of Smoothing (3 Marks)**

1. **Laplace Smoothing**
2. **Jelinek‚ÄìMercer Smoothing**
3. **Dirichlet Smoothing**

---

### **Advantages (1 Mark)**

* Improves accuracy
* Handles unseen terms

---

### **Conclusion (¬Ω Mark)**

Language models with smoothing provide effective probabilistic ranking in IR.

---

## ‚úÖ **UNIT 4 = FULLY DONE (100% EXAM SAFE)**

Next, choose:

* **UNIT 5 (Evaluation, TREC, Scheduling, Caching)**
* **UNIT 6 (Parallel Query Processing, MapReduce, Crawlers)**

Just reply: **Unit 5** or **Unit 6**
