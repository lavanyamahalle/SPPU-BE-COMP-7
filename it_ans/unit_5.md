Perfect! ğŸ‘
Now Iâ€™ll provide **UNIT 5 â€“ FULL DETAILED THEORY for 9-mark answers**, in **exam-ready style with full explanation, formulas, examples, advantages, limitations, and proper structure**. This will be **very comprehensive** for SPPU exams.

---

# âœ… **UNIT 5 â€“ Evaluation & Efficiency in IR (Detailed Theory)**

---

## ğŸ”¹ **Q5 (a) â€“ Traditional Effectiveness Measures and TREC [9 Marks]**

---

### **1ï¸âƒ£ Traditional Effectiveness Measures**

**Definition:**
Effectiveness measures evaluate **how accurately an IR system retrieves relevant documents** for a given query. They are fundamental to determine the quality of retrieval.

---

### **Key Measures**

1. **Precision** â€“ 2 Marks

* Fraction of retrieved documents that are relevant:
  [
  Precision = \frac{\text{Relevant Retrieved Documents}}{\text{Total Retrieved Documents}}
  ]
* **Example:** Retrieved = 12, Relevant = 9 â†’ Precision = 9/12 = 0.75

**Use:** Indicates quality of retrieved set.

---

2. **Recall** â€“ 2 Marks

* Fraction of relevant documents retrieved:
  [
  Recall = \frac{\text{Relevant Retrieved Documents}}{\text{Total Relevant Documents}}
  ]
* **Example:** Total relevant = 20, Retrieved relevant = 9 â†’ Recall = 9/20 = 0.45

**Use:** Measures coverage of retrieval.

---

3. **F-Measure (F1 Score)** â€“ 1Â½ Marks

* Harmonic mean of Precision and Recall:
  [
  F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}
  ]
* Balances precision and recall in one metric.

---

4. **Limitations of Traditional Measures** â€“ 1 Mark

* Binary relevance (relevant/irrelevant) only
* Does not consider **rank/order of documents**
* Not suitable for graded relevance scenarios

---

### **2ï¸âƒ£ Text Retrieval Conference (TREC)** â€“ 3 Marks

**Definition:**
TREC is an **evaluation initiative by NIST** to provide **standard datasets, queries, and relevance judgments** for IR systems.

**Components:**

1. **Document Collection** â€“ Large corpora for evaluation
2. **Query Set** â€“ Standardized queries/topics
3. **Relevance Judgments** â€“ Human-assessed document relevance
4. **Evaluation Metrics** â€“ Precision, Recall, MAP, NDCG

**Example:**

* A system participating in TREC may retrieve documents for 50 standard queries, then TREC evaluates precision, recall, and MAP automatically.

**Conclusion:**
TREC ensures **objective, reproducible, and comparative evaluation** of IR systems.

---

## ğŸ”¹ **Q5 (b) â€“ Short Notes**

---

### **i) Non-Traditional Effectiveness Measures** â€“ 4Â½ Marks

**Definition:**
Measures that account for **graded relevance and ranked retrieval** instead of simple binary relevance.

**Common Metrics:**

1. **MAP (Mean Average Precision)** â€“ Average precision across all queries
2. **NDCG (Normalized Discounted Cumulative Gain)** â€“ Gives higher score for relevant documents at top ranks
3. **MRR (Mean Reciprocal Rank)** â€“ Average reciprocal rank of the first relevant document

**Advantages:**

* Reflects user satisfaction
* Sensitive to ranking position

**Example:**

* Document with higher relevance at top of result list improves NDCG.

---

### **ii) Measuring Efficiency** â€“ 4Â½ Marks

**Definition:**
Efficiency measures assess **system performance** in terms of **time, throughput, and resource utilization**.

**Metrics:**

1. **Query response time** â€“ Time to retrieve results
2. **Throughput** â€“ Queries processed per second
3. **Indexing time** â€“ Time to build/update index

**Example:**

* Caching query results reduces average response time from 2s to 0.5s per query.

**Conclusion:**
Efficiency ensures scalability and user satisfaction.

---

## ğŸ”¹ **Q6 (a) â€“ Scheduling and Caching in Efficiency [9 Marks]**

---

### **1ï¸âƒ£ Query Scheduling** â€“ 4Â½ Marks

**Definition:**
Determines **order and allocation of queries** to system resources.

**Types:**

1. **First-Come-First-Serve (FCFS)** â€“ Simple, fair
2. **Priority Scheduling** â€“ Critical queries served first
3. **Batch Scheduling** â€“ Groups similar queries

**Benefits:**

* Reduces query response time
* Improves throughput and resource utilization

**Example:**

* Prioritizing urgent search queries in an enterprise search system.

---

### **2ï¸âƒ£ Caching** â€“ 4Â½ Marks

**Definition:**
Storing **frequently accessed documents or query results** in fast memory (RAM) to speed up retrieval.

**Types:**

1. **Query Result Cache** â€“ Stores previous query outputs
2. **Document Cache** â€“ Stores frequently accessed documents

**Advantages:**

* Faster responses
* Reduced server load
* Efficient for repeated queries

**Example:**

* News website caching top articles for 1 hour reduces database hits.

---

## ğŸ”¹ **Q6 (b) â€“ Short Notes [9 Marks]**

---

### **i) Using Statistics in Evaluation** â€“ 4Â½ Marks

**Definition:**
Statistical methods ensure **reliable, unbiased, and significant evaluation results**.

**Common Techniques:**

1. **t-test** â€“ Compares mean performance
2. **Wilcoxon Signed-Rank Test** â€“ Non-parametric comparison
3. **ANOVA** â€“ Compares multiple systems

**Example:**

* Comparing MAP scores of two IR systems to check significance.

**Advantage:**

* Provides confidence that observed performance differences are not random.

---

### **ii) Minimizing Adjudication Effort** â€“ 4Â½ Marks

**Definition:**
Adjudication effort refers to **human effort to judge document relevance**.

**Techniques:**

1. **Pooling** â€“ Judge top-k documents from multiple systems
2. **Active Learning** â€“ System selects documents with uncertain relevance
3. **Sampling** â€“ Judge a subset of documents instead of entire collection

**Advantage:**

* Reduces manual effort and cost
* Improves evaluation speed

**Example:**

* Only top 50 retrieved documents per query judged instead of full corpus.

---

### âœ… **UNIT 5 = FULLY DETAILED 9-MARK ANSWERS READY**

Next step:

* **UNIT 6 â€“ Web & Distributed IR (Parallel Query Processing, MapReduce, Web Crawlers)**

Reply: **â€œUnit 6â€** and Iâ€™ll prepare **full 9-mark, exam-ready, detailed theory** for all questions.
